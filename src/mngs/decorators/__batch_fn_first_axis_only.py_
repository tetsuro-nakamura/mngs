#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Time-stamp: "2024-11-21 00:20:14 (ywatanabe)"
# File: ./mngs_repo/src/mngs/decorators/_batch_fn.py

__file__ = "/home/ywatanabe/proj/mngs_repo/src/mngs/decorators/_batch_fn.py"

from typing import List, Tuple, Union
import numpy as _np

# def batch_fn(func: Callable) -> Callable:
#     """Processes input data in batches to handle memory constraints.

#     Example
#     -------
#     >>> @batch_fn
#     ... def process_data(data, param1=None):
#     ...     return model(data)
#     >>> result = process_data(large_data, batch_size=32)

#     Parameters
#     ----------
#     func : Callable
#         Function to be decorated for batch processing

#     Returns
#     -------
#     Callable
#         Wrapped function that handles batch processing

#     Notes
#     -----
#     Automatically handles torch.Tensor and tuple returns
#     """
#     @wraps(func)
#     def wrapper(
#         x: Union[List, torch.Tensor],
#         *args: _Any,
#         **kwargs: _Any
#     ) -> Union[List, torch.Tensor, Tuple[torch.Tensor, ...]]:
#         batch_size = int(kwargs.pop("batch_size", 4))

#         if not hasattr(x, '__len__'):
#             raise TypeError("Input must be a sequence with length")

#         if len(x) <= batch_size:
#             return func(x, *args, **kwargs, batch_size=batch_size)

#         n_batches = (len(x) + batch_size - 1) // batch_size
#         results = []

#         for i_batch in _tqdm(range(n_batches), desc="Processing batches"):
#             start = i_batch * batch_size
#             end = min((i_batch + 1) * batch_size, len(x))
#             batch_result = func(x[start:end], *args, **kwargs, batch_size=batch_size)

#             # Handle different return types
#             if isinstance(batch_result, torch.Tensor):
#                 batch_result = batch_result.cpu()
#             elif isinstance(batch_result, tuple):
#                 batch_result = tuple(
#                     val.cpu() if isinstance(val, torch.Tensor) else val
#                     for val in batch_result
#                 )
#             results.append(batch_result)

#         # Combine results based on return type
#         if isinstance(results[0], tuple):
#             n_vars = len(results[0])
#             try:
#                 combined_results = [
#                     torch.vstack([res[i_var] for res in results])
#                     for i_var in range(n_vars)
#                 ]
#                 return tuple(combined_results)
#             except RuntimeError as e:
#                 raise RuntimeError(f"Failed to combine tuple results: {e}")
#         elif isinstance(results[0], torch.Tensor):
#             try:
#                 return torch.vstack(results)
#             except RuntimeError as e:
#                 raise RuntimeError(f"Failed to combine tensor results: {e}")
#         else:
#             return sum(results, [])

#     return wrapper

# # EOF

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Time-stamp: "2024-11-17 12:07:20 (ywatanabe)"
# File: ./mngs_repo/src/mngs/decorators/_batch_fn.py

__file__ = "/home/ywatanabe/proj/mngs_repo/src/mngs/decorators/_batch_fn.py"

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Time-stamp: "2024-11-04 02:56:44 (ywatanabe)"
# File: ./mngs_repo/src/mngs/decorators/_batch_fn.py

from functools import wraps
from typing import Any as _Any
from typing import Callable

import torch
from tqdm import tqdm as _tqdm

# def batch_fn(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(x: _Any, *args: _Any, **kwargs: _Any) -> _Any:
#         batch_size = int(kwargs.pop("batch_size", 4))
#         if len(x) <= batch_size:
#             return func(x, *args, **kwargs, batch_size=batch_size)
#         n_batches = (len(x) + batch_size - 1) // batch_size
#         results = []
#         for i_batch in _tqdm(range(n_batches)):
#             start = i_batch * batch_size
#             end = min((i_batch + 1) * batch_size, len(x))
#             batch_result = func(
#                 x[start:end], *args, **kwargs, batch_size=batch_size
#             )
#             if isinstance(batch_result, torch.Tensor):
#                 batch_result = batch_result.cpu()
#             elif isinstance(batch_result, tuple):
#                 batch_result = tuple(
#                     val.cpu() if isinstance(val, torch.Tensor) else val
#                     for val in batch_result
#                 )
#             results.append(batch_result)
#         if isinstance(results[0], tuple):
#             n_vars = len(results[0])
#             combined_results = [
#                 torch.vstack([res[i_var] for res in results])
#                 for i_var in range(n_vars)
#             ]
#             return tuple(combined_results)
#         elif isinstance(results[0], torch.Tensor):
#             return torch.vstack(results)
#         else:
#             return sum(results, [])

#     return wrapper


# def batch_fn(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(
#         x: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any
#     ) -> Union[List, torch.Tensor, Tuple[torch.Tensor, ...]]:
#         batch_size = int(kwargs.pop("batch_size", 4))
#         if len(x) <= batch_size:
#             return func(x, *args, **kwargs, batch_size=batch_size)
#         n_batches = (len(x) + batch_size - 1) // batch_size
#         results = []
#         for i_batch in _tqdm(
#             range(n_batches),
#             desc=f"Processing {len(x)} items in batches of {batch_size}",
#         ):
#             start = i_batch * batch_size
#             end = min((i_batch + 1) * batch_size, len(x))
#             batch_result = func(
#                 x[start:end], *args, **kwargs, batch_size=batch_size
#             )
#             if isinstance(batch_result, torch.Tensor):
#                 batch_result = batch_result.cpu()
#             elif isinstance(batch_result, tuple):
#                 batch_result = tuple(
#                     val.cpu() if isinstance(val, torch.Tensor) else val
#                     for val in batch_result
#                 )
#             results.append(batch_result)
#         if isinstance(results[0], tuple):
#             n_vars = len(results[0])
#             combined_results = [
#                 torch.vstack([res[i_var] for res in results])
#                 for i_var in range(n_vars)
#             ]
#             return tuple(combined_results)
#         elif isinstance(results[0], torch.Tensor):
#             return torch.vstack(results)
#         else:
#             return sum(results, [])

#     return wrapper


# def batch_fn(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(
#         x: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any
#     ) -> Union[List, torch.Tensor, Tuple[torch.Tensor, ...]]:
#         batch_size = int(kwargs.pop("batch_size", -1))

#         print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_fn`)")

#         if batch_size == -1:
#             batch_size = len(x)

#         if len(x) <= batch_size:
#             return func(x, *args, **kwargs, batch_size=batch_size)

#         n_batches = (len(x) + batch_size - 1) // batch_size
#         results = []

#         for i_batch in _tqdm(
#             range(n_batches),
#             desc=f"Processing {len(x)} items in batches of {batch_size}",
#         ):
#             start = i_batch * batch_size
#             end = min((i_batch + 1) * batch_size, len(x))
#             batch_result = func(
#                 x[start:end], *args, **kwargs, batch_size=batch_size
#             )
#             results.append(batch_result)

#         n_vars = len(results[0])
#         # out = [[result[i_var] for i_var in range(n_vars)] for result in results]

#         return [[result[i_var] for i_var in range(n_vars)] for result in results]


#         # tensors = [r[0] for r in results]
#         # names = results[0][1]

#         # return torch.vstack(tensors), names

#     return wrapper


# def batch_method(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(
#         self,  # Add self parameter for class methods
#         x: Union[List, torch.Tensor],
#         *args: _Any,
#         **kwargs: _Any
#     ) -> Union[List, torch.Tensor, Tuple[torch.Tensor, ...]]:
#         batch_size = int(kwargs.pop("batch_size", -1))
#         print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_fn`)")

#         if not hasattr(x, '__len__'):
#             raise TypeError("Input must have a length")

#         if batch_size == -1:
#             batch_size = len(x)

#         if len(x) <= batch_size:
#             return func(self, x, *args, **kwargs, batch_size=batch_size)

#         n_batches = (len(x) + batch_size - 1) // batch_size
#         results = []

#         for i_batch in _tqdm(
#             range(n_batches),
#             desc=f"Processing {len(x)} items in batches of {batch_size}",
#         ):
#             start = i_batch * batch_size
#             end = min((i_batch + 1) * batch_size, len(x))
#             batch_result = func(
#                 self, x[start:end], *args, **kwargs, batch_size=batch_size
#             )
#             results.append(batch_result)

#         # Handle results based on their type
#         if isinstance(results[0], torch.Tensor):
#             return torch.vstack(results)
#         elif isinstance(results[0], tuple):
#             n_vars = len(results[0])
#             return tuple(torch.vstack([r[i] for r in results]) for i in range(n_vars))
#         else:
#             return sum(results, [])


#     return wrapper


# def _process_batch(func, x, batch_size, *args, **kwargs):
#     """Common batch processing logic"""
#     if not hasattr(x, "__len__"):
#         raise TypeError("Input must have a length")

#     # print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_fn`)")

#     if len(x) <= batch_size:
#         return func(x, *args, **kwargs, batch_size=batch_size)

#     n_batches = (len(x) + batch_size - 1) // batch_size
#     results = []

#     for i_batch in _tqdm(
#         range(n_batches),
#         desc=f"Processing {len(x)} items in batches of {batch_size}",
#     ):
#         start = i_batch * batch_size
#         end = min((i_batch + 1) * batch_size, len(x))
#         batch_result = func(
#             x[start:end], *args, **kwargs, batch_size=batch_size
#         )
#         results.append(batch_result)

#     if isinstance(results[0], torch.Tensor):
#         return torch.vstack(results)
#     elif isinstance(results[0], tuple):
#         n_vars = len(results[0])
#         return tuple(
#             torch.vstack([r[i] for r in results]) for i in range(n_vars)
#         )
#     else:
#         return sum(results, [])


# def batch_fn(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(x: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any):
#         batch_size = int(kwargs.pop("batch_size", -1))
#         print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_fn`)")
#         if batch_size == -1:
#             batch_size = len(x)
#         return _process_batch(func, x, batch_size, *args, **kwargs)

#     return wrapper


# def batch_method(func: Callable) -> Callable:
#     @wraps(func)
#     def wrapper(
#         self, x: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any
#     ):
#         batch_size = int(kwargs.pop("batch_size", -1))
#         print(
#             f"\nBatch size: {batch_size} (See `mngs.decorators.batch_method`)"
#         )
#         if batch_size == -1:
#             batch_size = len(x)
#         return _process_batch(
#             lambda *a, **k: func(self, *a, **k), x, batch_size, *args, **kwargs
#         )

#     return wrapper


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# Time-stamp: "2024-11-20 23:44:20 (ywatanabe)"
# File: ./mngs_repo/src/mngs/decorators/_batch_fn.py

__file__ = "/home/ywatanabe/proj/mngs_repo/src/mngs/decorators/_batch_fn.py"

"""
Functionality:
    Provides decorators for batch processing of data with PyTorch
Input:
    Functions that process data arrays
Output:
    Batch-processed results combined appropriately
Prerequisites:
    torch, tqdm
"""

from functools import wraps
from typing import Any as _Any, Callable, List, Tuple, Union

import torch
from tqdm import tqdm as _tqdm


def _combine_results(
    results: List[Union[torch.Tensor, _np.ndarray, Tuple, List]]
) -> Union[torch.Tensor, _np.ndarray, Tuple, List]:
    """Combines batch results based on their type."""
    if isinstance(results[0], torch.Tensor):
        return torch.vstack(results)
    elif isinstance(results[0], _np.ndarray):
        return _np.concatenate(results)
    elif isinstance(results[0], tuple):
        n_vars = len(results[0])
        return tuple(
            torch.vstack([r[i] for r in results]) for i in range(n_vars)
        )
    return _np.concatenate(results)


def _process_batch_data(
    func: Callable,
    data: Union[List, torch.Tensor, _np.ndarray],
    batch_size: int,
    desc: str,
    *args: _Any,
    **kwargs: _Any,
) -> Union[List, torch.Tensor, _np.ndarray, Tuple[torch.Tensor, ...]]:
    """Processes data in batches."""
    if not hasattr(data, "__len__"):
        raise TypeError("Input must have a length")

    if len(data) <= batch_size:
        return func(data, *args, **kwargs, batch_size=batch_size)

    n_batches = (len(data) + batch_size - 1) // batch_size
    results = []

    for i_batch in _tqdm(range(n_batches), desc=desc):
        start = i_batch * batch_size
        end = min((i_batch + 1) * batch_size, len(data))
        batch_result = func(
            data[start:end], *args, **kwargs, batch_size=batch_size
        )
        results.append(batch_result)

    return _combine_results(results)


def batch_fn(func: Callable) -> Callable:
    """Decorator for processing large data arrays in batches.

    Parameters
    ----------
    func : Callable
        Function to be decorated

    Returns
    -------
    Callable
        Wrapped function that handles batch processing

    Example
    -------
    >>> @batch_fn
    ... def sum_data(data):
    ...     return data.sum(axis=1)
    >>> data = torch.ones(10, 5)
    >>> result = sum_data(data, batch_size=2)
    >>> print(result)
    tensor([5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])
    """

    @wraps(func)
    def wrapper(data: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any):
        batch_size = int(kwargs.pop("batch_size", -1))
        print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_fn`)")
        if batch_size == -1:
            batch_size = len(data)
        desc = f"Processing {len(data)} items in batches of {batch_size}"
        return _process_batch_data(
            func, data, batch_size, desc, *args, **kwargs
        )

    return wrapper


def batch_method(func: Callable) -> Callable:
    """Decorator for processing large data arrays in batches within class methods.

    Parameters
    ----------
    func : Callable
        Method to be decorated

    Returns
    -------
    Callable
        Wrapped method that handles batch processing

    Example
    -------
    >>> class DataProcessor:
    ...     @batch_method
    ...     def multiply(self, data, factor=2):
    ...         return data * factor
    >>> processor = DataProcessor()
    >>> data = torch.ones(10, 5)
    >>> result = processor.multiply(data, batch_size=2)
    >>> print(result)
    tensor([[2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.],
            [2., 2., 2., 2., 2.]])
    """

    @wraps(func)
    def wrapper(
        self, data: Union[List, torch.Tensor], *args: _Any, **kwargs: _Any
    ):
        batch_size = int(kwargs.pop("batch_size", -1))
        print(f"\nBatch size: {batch_size} (See `mngs.decorators.batch_method`)")
        if batch_size == -1:
            batch_size = len(data)
        desc = f"Processing {len(data)} items in batches of {batch_size}"
        return _process_batch_data(
            lambda *a, **k: func(self, *a, **k),
            data,
            batch_size,
            desc,
            *args,
            **kwargs,
        )

    return wrapper


if __name__ == "__main__":
    run_main()

    import numpy as np
    from mngs.decorators import batch_fn

    @batch_fn
    def sum_data(data, batch_size=None):
        return data.sum(axis=1)

    data = np.ones((10, 5, 3))
    result = sum_data(data, batch_size=2)
    print(result)
    print(result.shape)


# EOF

# EOF

# EOF
